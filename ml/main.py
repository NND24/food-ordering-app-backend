from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from transformers import (
    AutoImageProcessor, 
    AutoModelForImageClassification,
    BlipProcessor,
    BlipForConditionalGeneration,
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
)
from PIL import Image
import requests
import torch
from pydantic import BaseModel
from typing import List, Optional
import pandas as pd
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.holtwinters import ExponentialSmoothing
import numpy as np
import logging
import io
import random

logger = logging.getLogger("uvicorn")

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


def clean_invalid_values(obj):
    if isinstance(obj, dict):
        return {k: clean_invalid_values(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [clean_invalid_values(v) for v in obj]
    elif isinstance(obj, (float, np.floating)):
        if np.isnan(obj) or np.isinf(obj):
            return 0.0
        return float(obj)
    else:
        return obj


# ---------------------- MODELS ----------------------
class AnalysisItem(BaseModel):
    period: str
    revenue: float
    cost: float
    profit: float
    margin: float
    growth: float

class ScenarioParams(BaseModel):
    trendChange: float = 0     # % thay ƒë·ªïi trend, v√≠ d·ª• 10 = +10%
    seasonalChange: float = 0  # % thay ƒë·ªïi seasonal
    costChange: float = 0      # % thay ƒë·ªïi chi ph√≠

class AnalyzeRequest(BaseModel):
    data: List[AnalysisItem]
    scenario: Optional[ScenarioParams] = None
    groupBy: Optional[str] = "day"  

# ---------------------- ROUTE ----------------------

@app.post("/analyze")
def analyze(req: AnalyzeRequest, period_type: str = "hour"):
    df = pd.DataFrame([item.dict() for item in req.data])
    df = df.sort_values("period")

    # ---- 1. Ph√¢n r√£ chu·ªói th·ªùi gian ----
    decomposition = {}
    ts = df["revenue"].astype(float)

    # X√°c ƒë·ªãnh chu k·ª≥ theo lo·∫°i period
    if period_type == "day":
        decomp_period = 24       # d·ªØ li·ªáu theo gi·ªù
    elif period_type == "week":
        decomp_period = 7        # d·ªØ li·ªáu theo ng√†y
    elif period_type == "month":
        if req.groupBy == "day":
            decomp_period = 30  # kho·∫£ng 30 ng√†y trong th√°ng
        elif req.groupBy == "week":
            decomp_period = 4   # 4 tu·∫ßn trong th√°ng
        else:
            decomp_period = 12  # theo th√°ng
    else:  # year
        decomp_period = 12      # 12 th√°ng

    try:
        if len(ts) < decomp_period * 2:
            trend = ts.rolling(window=max(2, len(ts)//2), min_periods=1).mean().fillna(0)
            seasonal = (ts - trend.rolling(window=2, min_periods=1).mean()).fillna(0)
            resid = (ts - trend - seasonal).fillna(0)
            decomposition = {
                "trend": trend.tolist(),
                "seasonal": seasonal.tolist(),
                "resid": resid.tolist(),
                "note": f"‚ö†Ô∏è Kh√¥ng ƒë·ªß d·ªØ li·ªáu ({len(ts)} ƒëi·ªÉm) ƒë·ªÉ ph√¢n r√£ theo chu k·ª≥ {decomp_period}, d√πng rolling mean thay th·∫ø.",
            }
        else:
            result = seasonal_decompose(ts, model="additive", period=decomp_period)
            decomposition = {
                "trend": result.trend.fillna(0).tolist(),
                "seasonal": result.seasonal.fillna(0).tolist(),
                "resid": result.resid.fillna(0).tolist(),
            }
    except Exception as e:
        decomposition = {"error": str(e)}

    # ---- 2. D·ª± ƒëo√°n n√¢ng c·∫•p v·ªõi seasonal ----
    forecast = {}
    try:
        # Ch·ªçn seasonal_type = "add" v√¨ decomposition d√πng additive
        model = ExponentialSmoothing(
            df["revenue"],
            trend="add",
            seasonal="add",
            seasonal_periods=decomp_period
        )
        model_fit = model.fit()
        pred_full = model_fit.fittedvalues 

        predicted_revenue_next = float(model_fit.forecast(1).tolist()[0])
        predicted_profit_next = predicted_revenue_next - float(df["cost"].iloc[-1])

        forecast = {
            "predictedRevenue": predicted_revenue_next,
            "predictedProfit": predicted_profit_next,
            "avgGrowth": df["revenue"].pct_change().mean() * 100,  # n·∫øu c·∫ßn %
            "predictedRevenueSeries": pred_full.tolist(),   # chu·ªói d·ª± ƒëo√°n
            "predictedProfitSeries": (pred_full - df["cost"]).tolist(),
        }
    except Exception as e:
        logger.exception("Forecast error")   # s·∫Ω in stacktrace
        forecast = {"error": str(e)}

    # ---- 3. Nh·∫≠n ƒë·ªãnh t·ª± ƒë·ªông (gi·ªëng c≈©) ----
    trend_mean = df["revenue"].diff().mean()
    trend_direction = "tƒÉng" if trend_mean > 0 else "gi·∫£m" if trend_mean < 0 else "·ªïn ƒë·ªãnh"
    seasonal_amplitude = abs(df["revenue"].max() - df["revenue"].min()) * 0.1
    seasonal_strength = "m·∫°nh" if df["revenue"].std() > seasonal_amplitude else "y·∫øu"

    insight_messages = []
    if trend_mean > 0:
        insight_messages.append("üìà Xu h∆∞·ªõng tƒÉng: doanh thu c√≥ chi·ªÅu h∆∞·ªõng ƒëi l√™n.")
        if trend_mean > 500:
            insight_messages.append("üöÄ M·ª©c tƒÉng m·∫°nh ‚Äî c√≥ th·ªÉ do marketing ho·∫∑c nhu c·∫ßu tƒÉng.")
    elif trend_mean < 0:
        insight_messages.append("üìâ Xu h∆∞·ªõng gi·∫£m: doanh thu c√≥ d·∫•u hi·ªáu ƒëi xu·ªëng.")
        if trend_mean < -500:
            insight_messages.append("‚ö†Ô∏è C·∫ßn xem l·∫°i gi√° b√°n ho·∫∑c chi·∫øn d·ªãch qu·∫£ng b√°.")
    else:
        insight_messages.append("‚û°Ô∏è Xu h∆∞·ªõng ·ªïn ƒë·ªãnh: doanh thu kh√¥ng thay ƒë·ªïi nhi·ªÅu.")

    if seasonal_strength == "m·∫°nh":
        insight_messages.append("üå§ M√πa v·ª• r√µ r·ªát: c√≥ giai ƒëo·∫°n cao ƒëi·ªÉm v√† th·∫•p ƒëi·ªÉm.")
        insight_messages.append("üí° G·ª£i √Ω: t·∫≠n d·ª•ng cao ƒëi·ªÉm ƒë·ªÉ ƒë·∫©y m·∫°nh khuy·∫øn m√£i.")
    else:
        insight_messages.append("üå§ M√πa v·ª• y·∫øu: doanh thu kh√° ƒë·ªÅu, √≠t b·ªã ·∫£nh h∆∞·ªüng th·ªùi ƒëi·ªÉm.")

    if forecast.get("predictedRevenue"):
        insight_messages.append(
            f"üîÆ D·ª± ƒëo√°n k·ª≥ t·ªõi: doanh thu {forecast['predictedRevenue']:,.0f} ‚Ç´, "
            f"l·ª£i nhu·∫≠n {forecast['predictedProfit']:,.0f} ‚Ç´."
        )

    # ---- 4. M√¥ ph·ªèng k·ªãch b·∫£n ng∆∞·ªùi d√πng (gi·ªëng c≈©) ----
    simulated_forecast = None
    scenario_insights = []

    if req.scenario:
        trend_factor = 1 + req.scenario.trendChange / 100
        seasonal_factor = 1 + req.scenario.seasonalChange / 100
        cost_factor = 1 + req.scenario.costChange / 100

        if "trend" in decomposition and "seasonal" in decomposition:
            simulated_trend = [t * trend_factor for t in decomposition["trend"]]
            simulated_seasonal = [s * seasonal_factor for s in decomposition["seasonal"]]
            simulated_series = [(t + s) for t, s in zip(simulated_trend, simulated_seasonal)]

            next_revenue = simulated_series[-1]
            next_cost = df["cost"].iloc[-1] * cost_factor
            next_profit = next_revenue - next_cost

            simulated_forecast = {
                "predictedRevenue": float(next_revenue),
                "predictedProfit": float(next_profit),
            }

            scenario_insights.append("üß© K·ªãch b·∫£n gi·∫£ l·∫≠p:")
            if req.scenario.trendChange != 0:
                direction = "tƒÉng" if req.scenario.trendChange > 0 else "gi·∫£m"
                scenario_insights.append(f"üìà Xu h∆∞·ªõng {direction} {abs(req.scenario.trendChange)}%.")
            if req.scenario.seasonalChange != 0:
                direction = "tƒÉng" if req.scenario.seasonalChange > 0 else "gi·∫£m"
                scenario_insights.append(f"üå§ M√πa v·ª• {direction} {abs(req.scenario.seasonalChange)}%.")
            if req.scenario.costChange != 0:
                direction = "tƒÉng" if req.scenario.costChange > 0 else "gi·∫£m"
                scenario_insights.append(f"üí∏ Chi ph√≠ {direction} {abs(req.scenario.costChange)}%.")
            scenario_insights.append(
                f"üí∞ K·∫øt qu·∫£ m√¥ ph·ªèng: doanh thu ~ {next_revenue:,.0f} ‚Ç´, "
                f"l·ª£i nhu·∫≠n ~ {next_profit:,.0f} ‚Ç´."
            )
        else:
            scenario_insights.append("‚ö†Ô∏è Kh√¥ng th·ªÉ m√¥ ph·ªèng do thi·∫øu d·ªØ li·ªáu decomposition.")

    response_data = {
        "decomposition": decomposition,
        "forecast": forecast,
        "insightMessages": insight_messages,
        "simulatedForecast": simulated_forecast,
        "scenarioInsights": scenario_insights,
    }

    return clean_invalid_values(response_data)

# ====== B∆Ø·ªöC 1: BLIP sinh m√¥ t·∫£ ti·∫øng Anh ======
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
model = BlipForConditionalGeneration.from_pretrained(
    "Salesforce/blip-image-captioning-large"
).to("cuda" if torch.cuda.is_available() else "cpu")

# ====== B∆Ø·ªöC 2: Model d·ªãch & vi·∫øt l·∫°i ti·∫øng Vi·ªát ======
translator_tokenizer = AutoTokenizer.from_pretrained("VietAI/envit5-translation")
translator_model = AutoModelForSeq2SeqLM.from_pretrained("VietAI/envit5-translation").to(
    "cuda" if torch.cuda.is_available() else "cpu"
)

def improve_vietnamese_caption(english_caption: str) -> str:
    prompt = f"Translate to Vietnamese: {english_caption}"
    inputs = translator_tokenizer(prompt, return_tensors="pt").to(translator_model.device)
    output = translator_model.generate(**inputs, max_new_tokens=100)
    vi_caption = translator_tokenizer.decode(output[0], skip_special_tokens=True)
    return vi_caption.strip()


@app.post("/caption")
def caption(url: str):
    # ==== B∆∞·ªõc 1: t·∫°o m√¥ t·∫£ ti·∫øng Anh ====
    image = Image.open(requests.get(url, stream=True).raw).convert("RGB")
    inputs = processor(image, return_tensors="pt").to(model.device)
    out = model.generate(**inputs, max_new_tokens=50)
    english_caption = processor.decode(out[0], skip_special_tokens=True)

    # ==== B∆∞·ªõc 2: d·ªãch & c·∫£i thi·ªán ====
    vietnamese_caption = improve_vietnamese_caption(english_caption)

    return {"caption_en": english_caption, "caption_vi": vietnamese_caption}


# AI sinh m√¥ t·∫£ t·ª´ ·∫£nh
try:
    from food_info import food_info
except ImportError:
    logger.warning("food_info module not found. Using empty dict.")
    food_info = {}
    
MODEL_CLS_NAME = "./finetuned_food_model" 
processor_cls = None
model_cls = None


def normalize_label(label: str):
    return label.lower().replace("-", " ").replace("_", " ").strip()

food_info_norm = {} # s·∫Ω ƒë∆∞·ª£c ƒëi·ªÅn khi startup

def classify_food(image_pil: Image.Image):
    """Ph√¢n lo·∫°i ·∫£nh v√† ch·ªâ tr·∫£ v·ªÅ k·∫øt qu·∫£ c√≥ ƒë·ªô ch√≠nh x√°c cao nh·∫•t."""
    if model_cls is None or processor_cls is None:
        raise HTTPException(status_code=503, detail="D·ªãch v·ª• model ch∆∞a s·∫µn s√†ng.")
        
    inputs = processor_cls(images=image_pil.convert("RGB"), return_tensors="pt")
    
    with torch.no_grad():
        outputs = model_cls(**inputs)
    
    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0]
    _, top_index = torch.max(probabilities, dim=-1)
    label = model_cls.config.id2label[top_index.item()]
    
    return label

# H√†m sinh m√¥ t·∫£ (T·ª´ code m·ªõi c·ªßa b·∫°n)
def generate_caption(label: str):
    label_norm = normalize_label(label)
    info = food_info_norm.get(label_norm) 

    if not info:
        # Fallback chung n·∫øu kh√¥ng c√≥ th√¥ng tin
        return [f"M√≥n {label} th∆°m ngon, h·∫•p d·∫´n, ch·∫Øc ch·∫Øn l√†m h√†i l√≤ng th·ª±c kh√°ch."]

    # 1. L·∫•y d·ªØ li·ªáu theo c·∫•u tr√∫c m·ªõi (ƒê√É C·∫¨P NH·∫¨T)
    display_name = info.get("display_name", label) 
    core_ingredients = info.get("core_ingredients", [])
    secondary_ingredients = info.get("secondary_ingredients", [])
    accompaniments = info.get("accompaniments", [])
    taste = info.get("taste", [])
    texture = info.get("texture", []) # L·∫•y d·ªØ li·ªáu texture m·ªõi
    style = info.get("style", [])
    
    # G·ªôp Taste v√† Texture v√†o m·ªôt list l·ªõn ƒë·ªÉ ch·ªçn ng·∫´u nhi√™n (Sensory Experience)
    all_sensations = taste + texture 

    # 2. Ki·ªÉm tra d·ªØ li·ªáu t·ªëi thi·ªÉu (ƒê√É C·∫¨P NH·∫¨T)
    # C·∫ßn √≠t nh·∫•t 1 core, 2 c·∫£m gi√°c (ƒë·ªÉ random 2 c√°i kh√°c nhau), 1 style
    if len(core_ingredients) == 0 or len(all_sensations) < 2 or len(style) == 0:
        # Fallback n·∫øu th√¥ng tin kh√¥ng ƒë·ªß ƒë·ªÉ t·∫°o c√¢u
        return [f"M√≥n {display_name} c√≥ th√¥ng tin phong ph√∫ v·ªÅ nguy√™n li·ªáu v√† h∆∞∆°ng v·ªã, l√† m·ªôt l·ª±a ch·ªçn tuy·ªát v·ªùi."]

    # 3. L·∫•y ng·∫´u nhi√™n taste/style (ƒê√É C·∫¨P NH·∫¨T: Ch·ªçn t·ª´ all_sensations)
    # C√°c bi·∫øn n√†y ƒë·∫°i di·ªán cho c·∫£m gi√°c/h∆∞∆°ng v·ªã t·ªïng th·ªÉ
    random_sensation_1 = random.choice(all_sensations)
    random_sensation_2 = random.choice([s for s in all_sensations if s != random_sensation_1]) 
    random_style_desc = random.choice(style) 
    
    # D√πng l·∫°i t√™n bi·∫øn c≈© cho g·ªçn
    random_taste_1 = random_sensation_1 
    random_taste_2 = random_sensation_2
    
    # 4. X·ª≠ l√Ω logic ingredients 
    core_ingredient_str = ", ".join(core_ingredients)
    all_extras_list = secondary_ingredients + accompaniments
    other_ingredients_str_t1_t2 = ""
    if len(all_extras_list) > 0:
        # L·∫•y 2 n·∫øu c√≥ th·ªÉ, kh√¥ng th√¨ l·∫•y 1
        k = min(len(all_extras_list), 2) 
        other_ingredients_str_t1_t2 = ", ".join(random.sample(all_extras_list, k))

    all_extras_str_t3 = ", ".join(all_extras_list)
    
    # 5. Th√™m logic ph√¢n bi·ªát "·∫®m th·ª±c" hay "Th·ª©c u·ªëng"
    label_norm = label.lower().replace('-', '') # C·∫ßn chu·∫©n h√≥a label tr∆∞·ªõc
    experience_type = "th·ª©c u·ªëng" if label_norm == "trasua" else "·∫©m th·ª±c"

    # 6. C·∫≠p nh·∫≠t Templates (ƒê√É S·ª¨A: D√πng "c·∫£m gi√°c" ƒë·ªÉ bao qu√°t c·∫£ v·ªã v√† k·∫øt c·∫•u)
    
    templates = [
        # Template 1: Thay "h∆∞∆°ng v·ªã... ƒë·∫∑c tr∆∞ng" b·∫±ng "c·∫£m gi√°c..."
        f"{display_name} l√† {random_style_desc}. {core_ingredient_str} l√† linh h·ªìn t·∫°o n√™n c·∫£m gi√°c {random_taste_1}." +
        (f" S·ª± k·∫øt h·ª£p ƒë∆∞·ª£c l√†m gi√†u b·ªüi {other_ingredients_str_t1_t2} mang l·∫°i tr·∫£i nghi·ªám {experience_type} kh√≥ qu√™n." if other_ingredients_str_t1_t2 else ""),
        
        # Template 2: Gi·ªØ nguy√™n (C·ª•m "c√¢n b·∫±ng tuy·ªát v·ªùi gi·ªØa c·∫£m gi√°c... v√† h∆∞∆°ng v·ªã..." ƒë√£ ho·∫°t ƒë·ªông t·ªët)
        f"Th∆∞·ªüng th·ª©c {display_name} l√† m·ªôt tr·∫£i nghi·ªám v·ªã gi√°c phong ph√∫. ƒêi·ªÉm ƒë·∫∑c s·∫Øc l√† {core_ingredient_str}" +
        (f" h√≤a quy·ªán c√πng {other_ingredients_str_t1_t2}" if other_ingredients_str_t1_t2 else "") +
        f", t·∫°o ra m·ªôt s·ª± c√¢n b·∫±ng tuy·ªát v·ªùi gi·ªØa c·∫£m gi√°c {random_taste_1} v√† h∆∞∆°ng v·ªã {random_taste_2}.",
        
        # Template 3: Gi·ªØ nguy√™n (S·ª≠ d·ª•ng "c·∫£m gi√°c")
        f"S·ª©c h·∫•p d·∫´n c·ªßa {display_name} ƒë·∫øn t·ª´ s·ª± ph·ª©c h·ª£p c·ªßa c√°c th√†nh ph·∫ßn. Ngo√†i {core_ingredient_str} l√† y·∫øu t·ªë c·ªët l√µi, " +
        (f"ƒë√¢y c√≤n l√† s·ª± k·∫øt h·ª£p nhu·∫ßn nhuy·ªÖn gi·ªØa {all_extras_str_t3}. " if all_extras_str_t3 else "") +
        f"T·ªïng th·ªÉ mang l·∫°i c·∫£m gi√°c {random_taste_1} v√† l√† ƒë·∫°i di·ªán cho {random_style_desc}."
    ]
    
    return random.choice(templates)

@app.on_event("startup")
async def load_resources():
    global processor_cls, model_cls, food_info_norm, food_info
    try:
        
        # 1. Load Model Ph√¢n lo·∫°i (S·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n c·ª•c b·ªô c·ªßa b·∫°n)
        print("\nƒêang t·∫£i model Ph√¢n lo·∫°i ·∫¢nh Finetuned Food Model...")
        processor_cls = AutoImageProcessor.from_pretrained(MODEL_CLS_NAME)
        model_cls = AutoModelForImageClassification.from_pretrained(MODEL_CLS_NAME)
        model_cls.eval() 
        
        # 2. Chu·∫©n h√≥a food_info
        food_info_norm = {normalize_label(k): v for k, v in food_info.items()}
        
        print("‚úÖ T·∫£i model v√† d·ªØ li·ªáu th√†nh c√¥ng.")
        
    except Exception as e:
        print(f"\n‚ùå L·ªñI KH√îNG TH·ªÇ T·∫¢I MODEL T·ª™ {MODEL_CLS_NAME}: {e}")
        logger.error(f"L·ªói t·∫£i model: {e}")
        model_cls = None

# ----------------------------------------------------------------------
# Endpoint API Ch√≠nh: Sinh M√¥ T·∫£ t·ª´ ·∫¢nh
# ----------------------------------------------------------------------

@app.post("/generate-caption-from-image", 
          response_model=dict, 
          summary="Ph√¢n lo·∫°i ·∫£nh v√† sinh ra m√¥ t·∫£ m√≥n ƒÉn")
async def generate_caption_from_image(file: UploadFile = File(..., description="File ·∫£nh m√≥n ƒÉn")):
    
    # 1. Ki·ªÉm tra lo·∫°i file
    if not file.content_type or not file.content_type.startswith("image/"):
        raise HTTPException(status_code=400, detail="File t·∫£i l√™n ph·∫£i l√† ·∫£nh.")
    
    try:
        # ƒê·ªçc ·∫£nh v√† chuy·ªÉn sang ƒë·ªëi t∆∞·ª£ng PIL
        image_bytes = await file.read()
        image_pil = Image.open(io.BytesIO(image_bytes))
        
        # 2. Ph√¢n lo·∫°i ·∫£nh
        prediction = classify_food(image_pil)
        
        if not prediction:
            raise HTTPException(status_code=500, detail="Kh√¥ng th·ªÉ d·ª± ƒëo√°n m√≥n ƒÉn t·ª´ ·∫£nh.")
        
        # 3. Sinh 3 m√¥ t·∫£ (ƒê√£ ƒë·ªïi t√™n h√†m)
        caption = generate_caption(prediction)
        
        # 4. Tr·∫£ v·ªÅ k·∫øt qu·∫£
        return {
            "success": True, 
            "prediction": prediction,
            "caption": caption
        }
    except HTTPException as h:
        raise h 
    except Exception as e:
        # logger.error(f"L·ªói x·ª≠ l√Ω request: {e}") 
        raise HTTPException(status_code=500, detail="L·ªói server khi ph√¢n lo·∫°i v√† sinh m√¥ t·∫£.")