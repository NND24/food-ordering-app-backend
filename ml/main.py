from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from transformers import (
    AutoImageProcessor, 
    AutoModelForImageClassification,
    BlipProcessor,
    BlipForConditionalGeneration,
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
)
from PIL import Image
import requests
import torch
from pydantic import BaseModel
from typing import List, Optional
import pandas as pd
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.holtwinters import ExponentialSmoothing
import numpy as np
import logging
import io
import random

logger = logging.getLogger("uvicorn")

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


def clean_invalid_values(obj):
    if isinstance(obj, dict):
        return {k: clean_invalid_values(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [clean_invalid_values(v) for v in obj]
    elif isinstance(obj, (float, np.floating)):
        if np.isnan(obj) or np.isinf(obj):
            return 0.0
        return float(obj)
    else:
        return obj


# ---------------------- MODELS ----------------------
class AnalysisItem(BaseModel):
    period: str
    revenue: float
    cost: float
    profit: float
    margin: float
    growth: float

class ScenarioParams(BaseModel):
    trendChange: float = 0     # % thay ƒë·ªïi trend, v√≠ d·ª• 10 = +10%
    seasonalChange: float = 0  # % thay ƒë·ªïi seasonal
    costChange: float = 0      # % thay ƒë·ªïi chi ph√≠

class AnalyzeRequest(BaseModel):
    data: List[AnalysisItem]
    scenario: Optional[ScenarioParams] = None
    groupBy: Optional[str] = "day"  

# ---------------------- ROUTE ----------------------
@app.post("/analyze")
def analyze(req: AnalyzeRequest, period_type: str = "hour"):
    df = pd.DataFrame([item.dict() for item in req.data])

    # -----------------------------
    # 0. Chu·∫©n ho√° th·ªùi gian
    # -----------------------------
    df["period"] = pd.to_datetime(df["period"], errors="coerce")
    df = df.sort_values("period")

    # -----------------------------
    # ‚≠ê 1. AUTO RESAMPLE (TƒÇNG DATAPOINT)
    # -----------------------------
    df = df.set_index("period")

    def auto_boost_datapoint(df):
        """
        TƒÉng s·ªë l∆∞·ª£ng datapoint b·∫±ng resample & interpolate tuy·∫øn t√≠nh.
        Th·ª≠ l·∫ßn l∆∞·ª£t 6H ‚Üí 3H ‚Üí 1H.
        """
        if len(df) >= 40:
            return df  # ƒë√£ ƒë·ªß nhi·ªÅu ‚Üí kh√¥ng c·∫ßn tƒÉng

        for freq in ["6h", "3h", "1h"]:
            boosted = df.resample(freq).interpolate(method="linear")
            if len(boosted) >= 40:   # ƒë·ªß datapoint ƒë·ªÉ decomposition
                return boosted

        return boosted  # d√πng b·∫£n cu·ªëi c√πng (1H)

    df = auto_boost_datapoint(df)
    ts = df["revenue"]

    # -----------------------------
    # ‚≠ê 2. T√çNH DECOMP_PERIOD
    # -----------------------------
    if period_type == "day":
        base_period = 24
    elif period_type == "week":
        base_period = 7
    elif period_type == "month":
        base_period = 30
    else:
        base_period = 12

    # N·∫øu boost datapoint l√™n ‚Üí chu k·ª≥ c·∫ßn scale l·∫°i
    # V√≠ d·ª•: ng√†y ‚Üí resample 6 gi·ªù ‚áí 1 ng√†y th√†nh 4 ƒëi·ªÉm
    inferred_points_per_day = int(24 / (df.index[1] - df.index[0]).total_seconds() * 3600)
    decomp_period = max(2, base_period * inferred_points_per_day // 24)

    # Gi·ªõi h·∫°n theo ƒë·ªô d√†i chu·ªói
    if len(ts) < decomp_period * 2:
        decomp_period = max(2, len(ts) // 3)

    # -----------------------------
    # ‚≠ê 3. PH√ÇN R√É CHU·ªñI (DECOMPOSE)
    # -----------------------------
    try:
        if len(ts) < 10:
            raise Exception("Not enough data for decomposition")

        result = seasonal_decompose(ts, model="additive", period=decomp_period)
        decomposition = {
            "trend": result.trend.fillna(0).tolist(),
            "seasonal": result.seasonal.fillna(0).tolist(),
            "resid": result.resid.fillna(0).tolist(),
            "periodUsed": decomp_period,
        }
    except Exception as e:
        # fallback: rolling
        trend = ts.rolling(window=max(2, len(ts)//2)).mean().fillna(0)
        seasonal = ts - trend.rolling(window=2, min_periods=1).mean().fillna(0)
        decomposition = {
            "trend": trend.tolist(),
            "seasonal": seasonal.tolist(),
            "resid": (ts - trend - seasonal).fillna(0).tolist(),
            "periodUsed": decomp_period,
            "note": f"Not enough data for full decomposition, using rolling instead: {str(e)}"
        }

    # -----------------------------
    # ‚≠ê 4. D·ª∞ B√ÅO (ExponentialSmoothing)
    # -----------------------------
    try:
        model = ExponentialSmoothing(
            df["revenue"],
            trend="add",
            seasonal="add",
            seasonal_periods=decomp_period
        )
        model_fit = model.fit()

        predicted_revenue_next = float(model_fit.forecast(1)[0])
        predicted_profit_next = predicted_revenue_next - float(df["cost"].iloc[-1])

        pred_full = model_fit.fittedvalues
        forecast = {
            "predictedRevenue": predicted_revenue_next,
            "predictedProfit": predicted_profit_next,
            "avgGrowth": df["revenue"].pct_change().mean() * 100,
            "predictedRevenueSeries": pred_full.tolist(),
            "predictedProfitSeries": (pred_full - df["cost"]).tolist(),
        }

    except Exception as e:
        forecast = {"error": str(e)}

    # -----------------------------
    # ‚≠ê 5. INSIGHTS 
    # -----------------------------
    trend_mean = df["revenue"].diff().mean()
    trend_direction = "tƒÉng" if trend_mean > 0 else "gi·∫£m" if trend_mean < 0 else "·ªïn ƒë·ªãnh"
    seasonal_strength = (
        "m·∫°nh" if df["revenue"].std() > abs(df["revenue"].max() - df["revenue"].min()) * 0.1 else "y·∫øu"
    )

    insight_messages = []
    if trend_mean > 0:
        insight_messages.append("Xu h∆∞·ªõng tƒÉng: doanh thu c√≥ chi·ªÅu h∆∞·ªõng ƒëi l√™n.")
        if trend_mean > 500:
            insight_messages.append("M·ª©c tƒÉng m·∫°nh ‚Äî c√≥ th·ªÉ do marketing ho·∫∑c nhu c·∫ßu tƒÉng.")
    elif trend_mean < 0:
        insight_messages.append("Xu h∆∞·ªõng gi·∫£m: doanh thu c√≥ d·∫•u hi·ªáu ƒëi xu·ªëng.")
        if trend_mean < -500:
            insight_messages.append("C·∫ßn xem l·∫°i gi√° b√°n ho·∫∑c chi·∫øn d·ªãch qu·∫£ng b√°.")
    else:
        insight_messages.append("Xu h∆∞·ªõng ·ªïn ƒë·ªãnh.")

    if seasonal_strength == "m·∫°nh":
        insight_messages.append("M√πa v·ª• r√µ r·ªát: c√≥ giai ƒëo·∫°n cao ƒëi·ªÉm ‚Äì th·∫•p ƒëi·ªÉm.")
    else:
        insight_messages.append("M√πa v·ª• y·∫øu: doanh thu kh√° ƒë·ªÅu.")

    if forecast.get("predictedRevenue"):
        insight_messages.append(
            f"üîÆ K·ª≥ t·ªõi: doanh thu {forecast['predictedRevenue']:,.0f} ‚Ç´, l·ª£i nhu·∫≠n {forecast['predictedProfit']:,.0f} ‚Ç´."
        )

    # -----------------------------
    # ‚≠ê 6. M√î PH·ªéNG K·ªäCH B·∫¢N (gi·ªØ logic)
    # -----------------------------
    simulated_forecast = None
    scenario_insights = []

    if req.scenario and "trend" in decomposition:
        trend_factor = 1 + req.scenario.trendChange / 100
        seasonal_factor = 1 + req.scenario.seasonalChange / 100
        cost_factor = 1 + req.scenario.costChange / 100

        simulated_series = [
            t * trend_factor + s * seasonal_factor
            for t, s in zip(decomposition["trend"], decomposition["seasonal"])
        ]

        next_revenue = simulated_series[-1]
        next_cost = df["cost"].iloc[-1] * cost_factor
        next_profit = next_revenue - next_cost

        simulated_forecast = {
            "predictedRevenue": float(next_revenue),
            "predictedProfit": float(next_profit),
        }

        scenario_insights.append("üß© K·ªãch b·∫£n gi·∫£ l·∫≠p:")
        if req.scenario.trendChange != 0:
            scenario_insights.append(f"üìà Xu h∆∞·ªõng thay ƒë·ªïi {req.scenario.trendChange}%.")
        if req.scenario.seasonalChange != 0:
            scenario_insights.append(f"üå§ M√πa v·ª• thay ƒë·ªïi {req.scenario.seasonalChange}%.")
        if req.scenario.costChange != 0:
            scenario_insights.append(f"üí∏ Chi ph√≠ thay ƒë·ªïi {req.scenario.costChange}%.")
        scenario_insights.append(
            f"üí∞ D·ª± b√°o: doanh thu {next_revenue:,.0f} ‚Ç´, l·ª£i nhu·∫≠n {next_profit:,.0f} ‚Ç´."
        )

    return clean_invalid_values({
        "decomposition": decomposition,
        "forecast": forecast,
        "insightMessages": insight_messages,
        "simulatedForecast": simulated_forecast,
        "scenarioInsights": scenario_insights,
    })


# ====== B∆Ø·ªöC 1: BLIP sinh m√¥ t·∫£ ti·∫øng Anh ======
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
model = BlipForConditionalGeneration.from_pretrained(
    "Salesforce/blip-image-captioning-large"
).to("cuda" if torch.cuda.is_available() else "cpu")

# ====== B∆Ø·ªöC 2: Model d·ªãch & vi·∫øt l·∫°i ti·∫øng Vi·ªát ======
translator_tokenizer = AutoTokenizer.from_pretrained("VietAI/envit5-translation")
translator_model = AutoModelForSeq2SeqLM.from_pretrained("VietAI/envit5-translation").to(
    "cuda" if torch.cuda.is_available() else "cpu"
)

def improve_vietnamese_caption(english_caption: str) -> str:
    prompt = f"Translate to Vietnamese: {english_caption}"
    inputs = translator_tokenizer(prompt, return_tensors="pt").to(translator_model.device)
    output = translator_model.generate(**inputs, max_new_tokens=100)
    vi_caption = translator_tokenizer.decode(output[0], skip_special_tokens=True)
    return vi_caption.strip()


@app.post("/caption")
def caption(url: str):
    # ==== B∆∞·ªõc 1: t·∫°o m√¥ t·∫£ ti·∫øng Anh ====
    image = Image.open(requests.get(url, stream=True).raw).convert("RGB")
    inputs = processor(image, return_tensors="pt").to(model.device)
    out = model.generate(**inputs, max_new_tokens=50)
    english_caption = processor.decode(out[0], skip_special_tokens=True)

    # ==== B∆∞·ªõc 2: d·ªãch & c·∫£i thi·ªán ====
    vietnamese_caption = improve_vietnamese_caption(english_caption)

    return {"caption_en": english_caption, "caption_vi": vietnamese_caption}


# AI sinh m√¥ t·∫£ t·ª´ ·∫£nh
try:
    from food_info import food_info
except ImportError:
    logger.warning("food_info module not found. Using empty dict.")
    food_info = {}
    
MODEL_CLS_NAME = "./finetuned_food_model" 
processor_cls = None
model_cls = None


def normalize_label(label: str):
    return label.lower().replace("-", " ").replace("_", " ").strip()

food_info_norm = {} # s·∫Ω ƒë∆∞·ª£c ƒëi·ªÅn khi startup

# H√†m classify (T·ª´ code m·ªõi c·ªßa b·∫°n)
def classify_food_topk(image_pil: Image.Image, top_k: int = 3):
    """Ph√¢n lo·∫°i ·∫£nh."""
    if model_cls is None or processor_cls is None:
        raise HTTPException(status_code=503, detail="D·ªãch v·ª• model ch∆∞a s·∫µn s√†ng.")
        
    inputs = processor_cls(images=image_pil.convert("RGB"), return_tensors="pt")
    
    with torch.no_grad():
        outputs = model_cls(**inputs)
    
    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)[0]
    topk_prob, topk_indices = torch.topk(probabilities, top_k)
    
    results = []
    for prob, index in zip(topk_prob.tolist(), topk_indices.tolist()):
        label = model_cls.config.id2label[index]
        results.append({"label": label, "score": round(prob, 4)})
        
    return results

# H√†m sinh m√¥ t·∫£ (T·ª´ code m·ªõi c·ªßa b·∫°n)
def generate_description(label: str):
    label_norm = normalize_label(label)
    info = food_info_norm.get(label_norm) 

    if not info:
        return [f"M√≥n {label} th∆°m ngon, h·∫•p d·∫´n, ch·∫Øc ch·∫Øn l√†m h√†i l√≤ng th·ª±c kh√°ch."]

    # ƒê·∫£m b·∫£o c√°c key t·ªìn t·∫°i
    display_name = info.get("display_name", label) 
    ingredients = info.get("ingredients", [])
    taste = info.get("taste", [])
    style = info.get("style", [])

    # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p thi·∫øu d·ªØ li·ªáu trong info
    if len(ingredients) < 2 or len(taste) < 3 or len(style) < 3:
         return [f"M√≥n {display_name} c√≥ th√¥ng tin phong ph√∫ v·ªÅ nguy√™n li·ªáu v√† h∆∞∆°ng v·ªã, l√† m·ªôt l·ª±a ch·ªçn tuy·ªát v·ªùi."]

    # L·∫•y ng·∫´u nhi√™n 2 gi√° tr·ªã t·ª´ taste/style (ƒë·∫£m b·∫£o ch√∫ng kh√°c nhau n·∫øu c·∫ßn)
    random_taste_1 = random.choice(taste)
    random_taste_2 = random.choice([t for t in taste if t != random_taste_1]) # ƒê·∫£m b·∫£o kh√°c nhau

    random_style_desc = random.choice(style) # L·∫•y ng·∫´u nhi√™n m·ªôt m√¥ t·∫£ phong c√°ch/s·ª≠ d·ª•ng

    # L·∫•y 2 th√†nh ph·∫ßn ph·ª• ng·∫´u nhi√™n
    other_ingredients = random.sample(ingredients[1:], 2)
    
    templates = [
        # Template 1: T·∫≠p trung v√†o m·ªôt y·∫øu t·ªë ng·∫´u nhi√™n
        f"{display_name} l√† {random_style_desc}. {ingredients[0]} l√† linh h·ªìn t·∫°o n√™n h∆∞∆°ng v·ªã {random_taste_1} ƒë·∫∑c tr∆∞ng. S·ª± k·∫øt h·ª£p ƒë∆∞·ª£c l√†m gi√†u b·ªüi {', '.join(other_ingredients)} mang l·∫°i tr·∫£i nghi·ªám ·∫©m th·ª±c/th·ª©c u·ªëng kh√≥ qu√™n.",
        
        # Template 2: Tr·∫£i nghi·ªám v√† s·ª± k·∫øt h·ª£p ng·∫´u nhi√™n
        f"Th∆∞·ªüng th·ª©c {display_name} l√† m·ªôt tr·∫£i nghi·ªám v·ªã gi√°c phong ph√∫. ƒêi·ªÉm ƒë·∫∑c s·∫Øc l√† {ingredients[0]} h√≤a quy·ªán c√πng {', '.join(other_ingredients)}, t·∫°o ra m·ªôt s·ª± c√¢n b·∫±ng tuy·ªát v·ªùi gi·ªØa c·∫£m gi√°c {random_taste_1} v√† h∆∞∆°ng v·ªã {random_taste_2}.",
        
        # Template 3: M√¥ t·∫£ T·ªïng quan v√† ƒê√°nh gi√° (S·ª≠ d·ª•ng t·∫•t c·∫£ c√°c th√†nh ph·∫ßn ph·ª• c√≤n l·∫°i)
        f"S·ª©c h·∫•p d·∫´n c·ªßa {display_name} ƒë·∫øn t·ª´ s·ª± ph·ª©c h·ª£p c·ªßa c√°c th√†nh ph·∫ßn. Ngo√†i {ingredients[0]} l√† y·∫øu t·ªë c·ªët l√µi, ƒë√¢y c√≤n l√† s·ª± k·∫øt h·ª£p nhu·∫ßn nhuy·ªÖn gi·ªØa {', '.join(ingredients[1:])}. T·ªïng th·ªÉ mang l·∫°i c·∫£m gi√°c {random_taste_1} v√† l√† ƒë·∫°i di·ªán cho {random_style_desc}."
    ]
    
    return templates

@app.on_event("startup")
async def load_resources():
    global processor_cls, model_cls, food_info_norm, food_info
    try:
        
        # 1. Load Model Ph√¢n lo·∫°i (S·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n c·ª•c b·ªô c·ªßa b·∫°n)
        print("\nƒêang t·∫£i model Ph√¢n lo·∫°i ·∫¢nh Finetuned Food Model...")
        processor_cls = AutoImageProcessor.from_pretrained(MODEL_CLS_NAME)
        model_cls = AutoModelForImageClassification.from_pretrained(MODEL_CLS_NAME)
        model_cls.eval() 
        
        # 2. Chu·∫©n h√≥a food_info
        food_info_norm = {normalize_label(k): v for k, v in food_info.items()}
        
        print("‚úÖ T·∫£i model v√† d·ªØ li·ªáu th√†nh c√¥ng.")
        
    except Exception as e:
        print(f"\n‚ùå L·ªñI KH√îNG TH·ªÇ T·∫¢I MODEL T·ª™ {MODEL_CLS_NAME}: {e}")
        logger.error(f"L·ªói t·∫£i model: {e}")
        model_cls = None

# ----------------------------------------------------------------------
# Endpoint API Ch√≠nh: Sinh M√¥ T·∫£ t·ª´ ·∫¢nh
# ----------------------------------------------------------------------

@app.post("/generate-caption-from-image", 
          response_model=dict, 
          summary="Ph√¢n lo·∫°i ·∫£nh v√† sinh ra 3 m√¥ t·∫£ m√≥n ƒÉn")
async def generate_caption(top_k: int = 1, file: UploadFile = File(..., description="File ·∫£nh m√≥n ƒÉn")):
    
    if not file.content_type or not file.content_type.startswith("image/"):
        raise HTTPException(status_code=400, detail="File t·∫£i l√™n ph·∫£i l√† ·∫£nh.")
    
    try:
        # ƒê·ªçc ·∫£nh v√† chuy·ªÉn sang ƒë·ªëi t∆∞·ª£ng PIL
        image_bytes = await file.read()
        image_pil = Image.open(io.BytesIO(image_bytes))
        
        # 1. Ph√¢n lo·∫°i ·∫£nh
        predictions = classify_food_topk(image_pil, top_k=top_k)
        
        if not predictions:
             raise HTTPException(status_code=500, detail="Kh√¥ng th·ªÉ d·ª± ƒëo√°n m√≥n ƒÉn t·ª´ ·∫£nh.")
        
        # 2. L·∫•y nh√£n d·ª± ƒëo√°n cao nh·∫•t
        best_label = predictions[0]['label']
        
        # 3. Sinh 3 m√¥ t·∫£
        descriptions = generate_description(best_label)
        
        # 4. Tr·∫£ v·ªÅ k·∫øt qu·∫£
        return {
            "success": True, 
            "best_prediction": predictions[0],
            "top_predictions": predictions,
            "descriptions": descriptions # Tr·∫£ v·ªÅ list 3 m√¥ t·∫£
        }
    except HTTPException as h:
        raise h 
    except Exception as e:
        logger.error(f"L·ªói x·ª≠ l√Ω request: {e}")
        raise HTTPException(status_code=500, detail="L·ªói server khi ph√¢n lo·∫°i v√† sinh m√¥ t·∫£.")